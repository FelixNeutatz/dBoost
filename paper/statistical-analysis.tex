\subsection{Statistical Analysis}
\label{sec:statistical-analysis}

Following the tuple expansion phase, the expanded tuples go through the statistical analysis phase. This phase collects simple statistics on each column of the table, and estimates which sets of columns are correlated.

The statistical information includes the average, variance, standard deviation and extremes of each numerical column, as well as approximate cardinality measurements for all columns. These statistics have two purposes. First, they are used at the end of the statistical analysis phase to determine which columns in the table are correlated. Second, the statistics can be used during data modeling to speed up the training phase of certain models by precompiling parameters needed by the models (e.g., mean and standard deviation for the Gaussian model).

We focus on two inter-column correlation detection strategies:

\begin{itemize}
\item For mostly-numeric datasets, we use Pearson's product-moment
  correlation. It relies on the Pearson correlation coefficient,
  which measures linear dependencies between two vectors.

  Given two column vectors $X$ and $Y$, Pearson's coefficient $R$ is given by the following formula:
  \begin{align}
    \label{eqn:pearson}
    R = \frac{\Covar(X,Y)}{\sqrt{\Var(X)\Var(Y)}}
  \end{align}

  $R$'s value always lies between $-1$ and $1$. An $R$ value close to 0 indicates little or no correlation, while values close to $+1$ or $-1$ indicate strong positive or negative correlations, respectively. Pairs of columns with a value of \(R\) above a user-specified threshold are added to a list of correlation hints, for use by the models.

\item For mostly non-numeric datasets, we use a cardinality-based measure, flagging groups of expanded columns as correlated when their joint cardinality is below a user-specified threshold. When two columns are correlated (e.g., when one is computed directly from the other), the number of distinct pairs in the columns is similar to the number of distinct items in either column. On the other hand, when two columns are independent, the number of distinct pairs is close to the product of the number of distinct values in each column.
\end{itemize}

It is debatable whether correlations should be looked for in expanded tuple fields that come from the same original value. On the one hand such dependencies may provide valuable insight about the data (think for example of a date whose ``day of week'' value would correlate with the month, for example for an event happening all Mondays of May and all Thursdays of June). On the other hand, taking these subtuple correlations into account vastly increases the size of the search space, and may add spurious hits to the results. Experimentally, we found that discarding intra-field correlations made the entire process faster and more robust, and did not hurt accuracy on our test sets.

All aforementioned statistics and correlation hints can be computed using a single pass over the data: the expanded tuples are analyzed one row at a time, and the final statistics and correlations are computed after the last tuple has been processed. This contrasts with more advanced approaches to the detection of correlations and soft functional dependencies, such as the one used in CORDS~\cite{Ilyas2004}. Our simpler approaches yields a lower specificity than CORDS on the datasets we evaluate in Section~\ref{sec:evaluation}, but this does not significantly impair the quality of the results. Indeed, each model only uses correlation hints as a guideline for interesting groups of columns to analyze. An excessive number of hints can affect performance, but does not significantly diminish the quality of the results. On the other hand, missing a correlation causes models to not analyze the corresponding group of columns, and thus to fail to uncover potential outliers.

The results of the analysis pass are available to all models used at later stages in the tool.
