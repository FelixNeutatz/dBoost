\subsection{Statistical Analysis}
\label{sec:statistical-analysis}

Following the tuple expansion phase, the newly expanded tuples go through a statistical analysis phase. This phase collects two kinds of information: simple statistics on each column's values, and data on possible inter-column correlations.

The statistical information includes the average, variance standard deviation and extremes of each numerical column, as well as approximate cardinality measurements for all columns. These statistics can then be used by models to speed up their training phase, or at the end of the statistical analysis to produce correlation hints.

Possible inter-column correlations can be detected in a number of ways; we focused on two strategies:

\begin{itemize}
\item For mostly-numeric datasets, we use Pearson's product-moment
  correlation. It relies on the Pearson correlation coefficient,
  which measures linear dependencies between two vectors.

  Given two column vectors $X$ and $Y$, Pearson's coefficient $R$ is given by the following formula:
  \begin{align}
    \label{eqn:pearson}
    R = \frac{\Covar(X,Y)}{\sqrt{\Var(X)\Var(Y)}}
  \end{align}

  $R$'s value always lies between $-1$ and $1$. An $R$ value close to 0 indicates little or no correlation, while values close to $+1$ or $-1$ indicate strong positive or negative correlations. Pairs of columns with a value of \(R\) above a user-specified threshold are added to a list of correlation hints, for use by models.
\item For mostly non-numeric datasets, we use a cardinality-based measure, flagging groups of expanded columns as correlated when the cardinality of their Cartesian product is below a user-specified threshold.
\end{itemize}

It is debatable whether correlations should be looked for in expanded tuple fields that come from the same original value. On the one hand such dependencies may provide valuable insight about the data (think for example of a date whose ``day of week'' value would correlate with the month, for example for an event happening all Mondays of May and all Thursdays of June). On the other hand, taking these subtuple correlations into account vastly increases the size of the search space, and may add spurious hits to the results. Experimentally, we found that discarding intra-field correlations made the entire process faster and more robust, and did not hurt accuracy on our test sets.

All aforementioned statistics and correlation hints can be computed using a single pass over the data: the expanded tuples are analyzed one row at a time, and the final statistics and correlations are computed after the last tuple has been processed. This contrasts with more advanced approaches to the detection of correlations and soft functional dependencies, such as the one used in \cite{Ilyas2004}.\fxnote{Check this} Our simpler approaches yields a lower specificity, but this does not significantly impair the quality of the results; indeed, each model only uses correlation hints as a guideline for interesting groups of columns to analyze; an excessive number of hints can affect performance, but does not significantly diminish the quality of the results. On the other hand, missing a correlation causes models to not analyze the corresponding group of columns, and thus to fail to uncover potential outliers.

The results of the analysis pass are available to models used at later stages in the tool.
