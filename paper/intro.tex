% INTRODUCTION

Sensor glitches, submission of incorrect input, and malicious activities are a few examples of events that can lead to the insertion of outlying values in a dataset. If undetected, these values can skew statistics, support invalid conclusions, slow database operations, cause otherwise avoidable expenses. On the other hand, careful analysis of these values can yield new insight about the data, prevent undesirable events, and generally improve the reliability of the data.

Previous literature has generally defined an outlier as ``an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism'' \cite{Hawkins1980}, and has proposed a number of ways to detect and in some cases eliminate suspicious values. Previous approaches to outlier detection include modeling numerical data using Gaussian Mixture Models~\cite{Lu2005,Roberts1994,Roberts1999}, Histogram modeling~\cite{Gebski2007,Sheng2007}, and $k$-nearest neighbors~\cite{Ramaswamy2000}.

Little work, however, has focused on developing generic methods for user-guided outlier detection, providing sufficient flexibility to offer insight on widely diverse, heterogeneous data stored in typical relational database management systems. The relative inexpressivity of basic SQL types -- integers, strings, and doubles in particular, might be to blame: strings, for example, can be used to store things as diverse as city names, email addresses, or phone numbers; the task rests on application logic to parse these values in and out of the databases, and outlier detection applications are left with very little information to work on.

This work present a novel approach to reporting and flagging outliers in highly heterogeneous datasets: our tools optimistically expand semantically poor SQL types to derive richer information, and use this derived information to detect outliers and provide detailed reports to the user. Expansion proceeds by applying a set of type-dependent rules, such that, for example, dates are reconstructed from integers by considering the as UNIX timestamps. Rules function mapping values of one type to a set of expanded data; they are user-configurable, and, when many rows in a database agree on a particular expanded value, can be used to express soft constraints on the data. For example, if most of the values in a string column have the same casing, then a soft constraint can be learned about the proper casing of values of that column.

We designed the framework to be both fast and memory efficient; it proceeds in three online passes over the data, keeping no more information than strictly necessary (in general, no more than a few dozens of machine words per field in the database schema). The architecture is parallelizable, the analysis can be distributed over multiple computation nodes, and information can be kept from one run to the next so as to eliminate the first and possibly the second pass.

Our contributions are as follows:
\begin{enumerate}
\item We discuss the idea of tuple expansion, and a set of rules that proved useful in flagging outliers in human-input data.
\item We describe a histogram-based approach in the context of detecting outliers in non-numerical, heterogeneous datasets.
\item We evaluate the performance of our framework on several real-world datasets.
\end{enumerate}

The rest of this paper is structured as follows:
\begin{itemize}
\item We first provide an overview of our framework in Section~\ref{sec:overview}.
\item In Section~\ref{sec:implementation}, we delve into the details of how we built a software tool to implement our framework.
\item We evaluate our tool on several real-world problems in Section~\ref{sec:eval}.
\item We describe related work in Section~\ref{sec:related_work}.
\item We outline interesting directions for future work in Section~\ref{sec:future}.
\item We conclude in Section~\ref{sec:concl}
\end{itemize}
