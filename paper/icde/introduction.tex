\section{Introduction}
\label{sec:intro}

Sensor glitches, data-entry errors, and malicious activities are a few examples of events that can lead to the appearance of outliers in a dataset. If undetected, these values can skew statistics, support invalid conclusions, slow database operations, and cause otherwise avoidable expenses. On the other hand, careful analysis of these values can yield new insight about the data, prevent undesirable events, and generally improve the reliability of the data~\cite{Achour2014}.

Previous literature has generally defined an outlier as ``an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism'' \cite{Hawkins1980}, and has suggested a number of ways to detect and in some cases eliminate suspicious values. Previous approaches to outlier detection include modeling numerical data using Gaussian Mixture Models~\cite{Lu2005,Roberts1994,Roberts1999}, Histogram modeling~\cite{Gebski2007,Sheng2007}, and $k$-nearest neighbors~\cite{Ramaswamy2000}.

Little work, however, has focused on developing generic methods for user-guided outlier detection that work with widely diverse and heterogeneous data stored in typical relational database management systems. The relative inexpressivity of basic SQL types -- integers, strings, and doubles in particular -- might be to blame: strings, for example, can be used to store information as diverse as city names, email addresses, or phone numbers; typically it is the job of application logic to encode the semantically rich domains of these values as one of the basic SQL data types. This paucity of semantic information leaves outlier detection algorithms that run inside of the database with very little information to work with.  

This paper presents a new approach for detecting outliers in highly heterogeneous datasets: our tools systematically expand the limited space of SQL types to derive richer information.
This is done automatically, by applying a library of possible transformations (which we call ``expansions'') to the values of each column in a table to find rules that are consistent with the bulk of the values that appear in the column.
For example, integers can expanded into dates and times by considering them as Unix timestamps, and into sets of booleans by considering them as bit vectors. 
If an expansion of an integer column to a the day of week from a date found that most of the dates reconstructed from an integer column fall on the same day of the week, then values falling on other days might be flagged as suspicious. 
As in this example, these expansions  can be used to detect outliers that are difficult or impossible to detect using raw data and provide detailed reports to the user. 

Hence, our main contribution is a method that automatically expansion rules (type-dependent transformations) to create a set of derived attributes for every tuple. Rules that best match the values appearing in each column are retained.
These derived attributes are then processed by several outlier detection models to efficiently learn soft constraints about the both individual attributes, and to detect soft functional dependencies \textit{between} derived attributes, enabling multidimensional models to detect of a broad class of data inconsistencies.

\srm{explain better how this method enables something that fundamentally wasn't possible before.}

\srm{describe non-contributions -- i.e., say that we know some of our continuous modeling techniques aren't novel, but that the contribution is in the overall system design and idea of tuple expansion, and in particular in our pruning and peakiness (or whatever we call them) tests for histograms.}

We designed our system to be both fast and memory efficient; it proceeds in three online passes over the data, keeping no more information than strictly necessary (in general, no more than a few dozen values per field in the database schema). The architecture is parallelizable, the analysis can be distributed over multiple computation nodes, and information can be kept from one run to the next so as to eliminate the first and possibly the second pass.

\srm{contributions should include more details -- i.e., pruning methods for histograms, automatic partitioning methods, peakiness tests for histograms, etc.}

In summary, our contributions are as follows:
\begin{enumerate}
\item We propose a method to provide semantically rich annotations, or \emph{expansions}, which automatically and efficiently extend columns, using a user-extensible library of possible expansions.
\item We show that these expansions can be fed to both single and multi-variate models to detect outliers.
%\item We discuss the idea of tuple expansion and a set of rules that prove useful in flagging outliers in heterogeneous datasets.
\item We show that these techniques can detect outliers on several synthetic and real-world datasets, including 
 numerical and non-numerical data and homo- and heterogeneous datasets.
%\item We propose a histogram-based approach to detect outliers in non-numerical, heterogeneous datasets.
\item 
\end{enumerate}

The rest of this paper is structured as follows: In Section~\ref{sec:overview} we present an overview in the framework. We then detail our tuple expansion method in Section~\ref{sec:expansion}, before applying it to outlier detection in Section~\ref{sec:implementation}. We evaluate our tool on synthetic and real-world datasets in Section~\ref{sec:evaluation}, and we describe related work in Section~\ref{sec:related-work}. Finally, we conclude in Section~\ref{sec:conclusion}. % TODO \item We outline directions for future work in Section~\ref{sec:future-work}.
